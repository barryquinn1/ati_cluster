{
  "hash": "f4725a95763edb6a544cdc86377d11ab",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Clustering in Finance\"\nsubtitle: \"A primer on partitional and hierarchical clustering algorithms, with applications to financial datasets\"\nauthor: \"Barry Quinn <br> Digital Dave\"\nembed-resources: true\nfooter: \"AI and Trading\"\nformat:\n  revealjs:\n    include-in-header:\n      - https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\n    theme: default\n    transition: slide\n    scollable: true\n    slide-number: true\n    logo: img/qbslogo.png\n    css: mycssblend.css\n    height: 900\n    width: 1600\n    html-math-method: mathml\nexecute: \n  echo: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n## Packages used\n\n```         \nlibrary(tidyverse)\nlibrary(fpc)\nlibrary(factoextra)\nlibrary(fungible)\nlibrary(ggdendro)\n```\n\n## Introduction\n\n-   Many problems in finance require the clustering of variables or\n    observations:\n    1.  Factor investing, relative value analysis (e.g., forming quality\n        minus junk portfolios)\n    2.  Risk management, portfolio construction (e.g., deriving the\n        efficient frontier)\n    3.  Dimensionality reduction (e.g., decomposing bond return drivers)\n    4.  Modelling of multicollinear systems (e.g., computing p-values)\n-   Despite its usefulness, clustering is almost never taught in\n    Econometrics courses\n-   None of the major Econometrics textbooks, and only a handful of\n    academic journal articles, discuss the clustering of financial\n    datasets\n\n## Learning outcomes\n\n-   Partitional clustering\n-   Hierarchical clustering\n-   Understand that different features and/or similarity metrics will\n    lead to different clusterings\n-   Understand it is key to formulate the problem in a way that results\n    have economic meaning and interpretability\n\n## What is Clustering? {.small}\n\n::: columns\n::: {.column width=\"50%\"}\n-   A clustering problem consists of a set of objects and a set of\n    features associated with those objects.\n-   The goal is to separate the objects into groups (called clusters)\n    using the features, where [intragroup]{.red} similarities are\n    maximized, and [intergroup similarities]{.red} are minimized.\n-   It is a form of unsupervised learning, because we do not provide\n    examples to assist the algorithm in solving this task.\n-   [Clustering problems appear naturally in finance, at every step of\n    the investment process.]{.blockquote}\n:::\n\n::: {.column width=\"50%\"}\n![](img/agglom.png)\n:::\n:::\n\n## Why cluster in finance? {.small}\n\n1.  Quantitative analysts may look for historical analogues to current\n    events, a task that involves developing a numerical taxonomy of\n    events.\n2.  Portfolio managers often cluster securities with respect to a\n    variety of features, to derive relative values among peers.\n3.  Risk managers are keen to avoid the concentration of risks in\n    securities that share common traits.\n4.  Traders wish to understand flows affecting a set of securities, to\n    determine whether a rally or sell-off is idiosyncratic to a\n    particular security, or affects a category shared by a multiplicity\n    of securities.\n\n[In tackling these problems, we use the notions of distance we studied\nin previous weeks]{.blockquote}\n\n## Proximity matrix {.small}\n\n-   Consider a data matrix $X$, of order $N \\times F$, where $N$ is the\n    number of objects and $F$ is the number of features.\n-   We use the $F$ features to compute the proximity between the N\n    objects, as represented by an $N \\times N$ matrix.\n-   The proximity measure can indicate either similarity (e.g.,\n    correlation, mutual information) or dissimilarity (e.g., a distance\n    metric).\n-   It is convenient but not strictly necessary that dissimilarity\n    measures satisfy the conditions of a metric: nonnegativity, symmetry\n    and triangle inequality\n-   The proximity matrix can be represented as an undirected graph where\n    the weights are a function of the similarity (the more similar, the\n    greater the weight) or dissimilarity (the more dissimilar, the\n    smaller the weight).\n-   Then the clustering problem is equivalent to breaking the graph into\n    connected components (disjoint connected subgraphs), one for each\n    cluster.\n-   When forming the proximity matrix, it is a good idea to standardize\n    the input data, to prevent that one feature's scale dominates over\n    the rest.\n\n## Cluster FIFA 20 players {.small}\n\n![](img/fifa20.png)\n\n## Clustering algorithms {.small}\n\n[There are two main classes of clustering algorithms: partitional and\nhierarchical]{.blockquote}\n\n-   Partitional techniques create a one-level (un-nested) partitioning\n    of the objects (each object belongs to one cluster, and to one\n    cluster only) simultaneously.\n-   Hierarchical techniques produce a nested sequence of partitions,\n    with a single, all-inclusive cluster at the top and singleton\n    clusters of individual points at the bottom. Hierarchical clustering\n    algorithms can be divisive (top-down) or agglomerative (bottom-up).\n\n## A partitional algorithm: K-means {.small}\n\n-   K-Means is a vector quantization model.\n\n### Statistical thinking {{< fa brain >}}\n\nIt attempts to split the samples (rows) of $X$ into a predetermined\nnumber of clusters $K$\n\n### Algorithmic thinking {{< fa brain >}} {{< fa plus >}} {{< fa brain >}}\n\n-   [Step 1: Initialize a random set of K centroids]{.saltinline}\n\n-   [Step 2: Assign each sample to one cluster such that the\n    within-cluster variance is minimised]{.fatinline}\n\n-   [Step 3: Update K centroids based on the clusters from Step\n    2]{.acidinline}\n\n-   [Step 4: Repeat steps 2 and 3 until convergence]{.heatinline}\\]\n\n::: panel-tabset\n## A\n\n## B\n:::\n\n## A partitional algorithm: K-mean {.small}\n\n::: panel-tabset\n## Experiment set up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfungible::monte(seed = 123, nvar = 2, nclus = 3, clus.size = c(1000, 1000, 1000), eta2 = c(0.70, 0.30)) -> dat\ndat1 <- as_tibble(dat$data)\nkmeansObj <- kmeans(dat1, centers = 3)\n```\n:::\n\n\n-   The [`fungible::monte()`](https://rdrr.io/cran/fungible/man/monte.html) simulates a set of clusters which have some proportion of total variance is due to their mixture.\n-   This is typical of a financial data set, where the common market component will affect all clusters.\n\n## Clustering results\n\n::: columns\n::: {.column width=\"50%\"}\n-   The algorithm took 4 before finding a solution\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nfactoextra::fviz_cluster(kmeansObj, data = dat1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n:::\n\n## A few considerations {.small}\n\n-   K-Means assumes that the clusters are convex, isotropic, and with\n    similar variance\n-   Features should be standardized prior to clustering\n-   Other algorithms may perform better when clusters are elongated or\n    irregular\n-   Within-cluster variance is not a normalized metric\n-   Curse of dimensionality: When $X$ has many columns, variances are\n    inflated, and outcomes may be biased.\n-   One solution is to apply a dimensionality reduction technique (e.g.,\n    PCA) prior to clustering\n-   K-Means will always converge, however the outcome may be a local\n    minimum\n-   One solution is to run multiple instances in parallel, with\n    different seed centroid\n\n## A Hierarchical Algorithm: Agglomerative Clustering {.small}\n\n### Algorithmic thinking {{<fa brain>}} {{<fa plus>}} {{<fa brain>}} \n- Step 1: Apply a distance metric to $X$\n- Step 2: Combine into a cluster the pair with lowest distance.  The pair can be composed of two items, two clusters, or one item and a cluster] \n- Step 3: Reduce the distance matrix\n  - Remove the 2 rows and columns associated with the pair \n  - Apply a linkage criterion to determine the distance between the new cluster and the rest of objects, e.g.: \n  -[Single linkage](https://en.wikipedia.org/wiki/Single-linkage_clustering): minimum distance to any object in the pair \n  - [Complete linkage](https://en.wikipedia.org/wiki/Complete-linkage_clustering): maximum distance to any object in the pair]{.small} \n- Step 4: Repeat steps 2 and 3 until the distance matrix has been reduced to only one object\n\n## Dendrogram {.small}\n\n-   A dendrogram is a tree graph that displays the hierarchical\n    composition of the clusters\n-   The y-axis indicates the distance between the two objects that form\n    a new cluster\n-   A linkage matrix characterizes a dendrogram\n    -   For N items, a linkage matrix has N-1 rows (one row per cluster)\n    -   [Three columns:]{.heatinlne}\n        -   Integer identifying object 1\n        -   Integer identifying object 2\n        -   Distance between objects 1 and 2 (based on linkage\n            criterion)\n-   pretty dendrograms = `ggdendro` package\n\n## A few considerations {{<fa brain>}} {.small}\n\n-   Hierarchical algorithms can handle clusters that are non-convex,\n    anisotropic, with unequal variance\n    -   This includes clusters within clusters\n-   Hierarchical algorithms allow connectivity constraints\n    -   Connectivity constraints cluster together only adjacent points.\n        This links together points even if the centroid is not part of\n        the cluster\n-   However, hierarchical algorithms may not handle properly elongated\n    blobs\n    -   One solution is to orthogonalize the features (e.g., PCA without\n        dimensionality reduction) prior to clustering\n-   The appropriate linkage method can be chosen via cross-validation,\n    or [cophenetic\n    correlation](https://en.wikipedia.org/wiki/Cophenetic_correlation)\n\n## Example of hierarchical clustering {.small}\n\n::: panel-tabset\n## Simulate three clusters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nf1 <- rnorm(45, rep(1:3, each = 15), 0.2)\nf2 <- rnorm(45, rep(c(1, 2, 1), each = 15), 0.2)\ntibble(x = f1, y = f2, obs = 1:45) -> dat\n\ndat %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_point(colour = 'pink') +\n  geom_label(aes(label = obs)) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n- The above code using a base R approach to clustering. Two features are drawn from a normal distribution, creating three clusters with some noise. The observations are each labelled from 1 to 45\n\n## hierarchical agglomerate clustering\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhClustering <- dat %>% \n  dist() %>% \n  hclust(method = 'single') #<<\n```\n:::\n\n\n- the function `hclust()` takes a distance matrix `dist` (default is euclidean distance) from the tibble `dat` and then\nderives a linkage matrix using a single-linkage criterion.\n\n-   Initially, each observation is assigned to its own cluster and then the algorithm proceeds iteratively, at each stage joining the two most similar clusters, continuing until there is just a single cluster. \n- At each stage distances between clusters are recomputed by the Lance–Williams dissimilarity update formula according to the particular clustering method being used. `?hclust()` for more\n\n\n## Dendrogram output\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nggdendrogram(hClustering)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n[ - By restricting the growth of a hierarchical tree, we can derive a\npartitional clustering from any hierarchical clustering. - However, one\ncannot generally derive a hierarchical clustering from a partitional\none. ]{.blockquote}\n:::\n:::\n\n:::\n\n## Types of clustering {.small}\n\n-   Depending on the definition of cluster, we can distinguish several\n    types of clustering algorithms, including the following:\n\n1.  [Connectivity:]{.saltinline} This clustering is based on distance\n    connectivity, like hierarchical clustering. Finance example =\n    @Prado2016.\n2.  [Centroids:]{.saltinline} These algorithms perform a vector\n    quantization, like k-means, Finance example = @Prado2018.\n3.  [Distribution:]{.saltinline} Clusters are formed using statistical\n    distributions\n4.  [Density:]{.saltinline} These algorithms search for connected dense\n    regions in the data space. Examples include [DBSCAN and\n    OPTICS](https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf).\n5.  [Subspace:]{.saltinline} Clusters are modeled on two dimensions,\n    features and observations. An example is biclustering/coclustering.\n    For instance, they can help identify similarities in subsets of\n    instruments and time periods\n\n## Cluster algorithm inputs\n\n-   Some algorithms expect as input a measure of similarity, and other\n    algorithms expect as input a measure of dissimilarity.\n-   It is important to make sure that you pass the right input to a\n    particular algorithm.\n-   For instance, a hierarchical clustering algorithm typically expects\n    distance as an input, and it will cluster together items within a\n    neighborhood.\n-   Centroids, distribution and density methods expect vector-space\n    coordinates, and they can handle distances directly.\n-   However, biclustering directly on the distance matrix will cluster\n    together the most distant elements (the opposite of what say k-means\n    would do). One solution is to bicluster on the reciprocal of\n    distance.\n\n## Curse of dimensionality\n\n- If the number of features greatly exceeds the number of\nobservations, the curse of dimensionality can make the clustering problematic: most of the space spanning the observations will be empty, making it difficult to identify any groupings. \n- One solution is to project the data matrix X onto a low-dimensional space, similar to how PCA reduces the number of features (Steinbach, 2004, Ding, 2004). \n- An alternative solution is to project the proximity matrix onto a low-dimensional space, and use it as a new X matrix. \n- In both cases, denoising and detoning can help identify the number of dimensions associated with signal.\n\n## Number of clusters\n\n-   Partitioning algorithms find the composition of unnested clusters, where the researcher is responsible for providing the correct numberof clusters.\n-   In practice, researchers often do not know in advance what the number of clusters should be.\n-   The \"elbow method\" is a popular technique that stops adding clusters when the marginal percentage of variance explained does not exceed a predefined threshold\n-   In this context, the percentage of variance explained is defined as the ratio of the between-group variance to the total variance (an F-test).\n-   One caveat of this approach is that the threshold is often set arbitrarily. (Goutte et al. 1999).\n\n## Optimal Number of Clusters (ONC)\n\n-   Lopez de Prado (2020) presents the ONC algorithm, which\n    recovers the number of clusters from a shuffled block-diagonal correlation matrix.\n\n-   ONC belongs to the broader class of algorithms that apply these[Silhouette method](https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/silhouette).\n\n-   Although we typically focus on finding the number of clusters withina correlation matrix, this algorithm can be applied to any generic observation matrix.\n\n## Cluster scoring {.small}\n\n-   In order to determine the optimal number of clusters, we first need\n    to define a function that scores the output of a scoring algorithm\n-   In general, there are two types of clustering scoring functions:\n\n1.  External: those that require ground-truth labels\n2.  Internal: those that don’t require it\n\n-   Because clustering is an unsupervised learning problem, internal\n    scores are more natural. Three of the most used internal scoring\n    functions are:\n\n1.  Calinski-Harabasz index (or variance ratio) **direct method**\n2.  Gap statistics **Statistical method**\n3.  Silhouette scores **direct method**\n\n## How many blobs are there? {.small}\n\n::: columns\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\nf2c3<-monte(seed = 123,nvar = 2,nclus = 3,clus.size = c(1000,1000,1000),eta2 = c(0.9,0.9))[['data']] %>% as.tibble()\nf2c3 %>% ggplot(aes(x=V2,y=V3)) + geom_point(colour='pink')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/2D 4 blobs-1.png){width=960}\n:::\n:::\n\n\n-   On 2-D, this is an easy question for a human.\n:::\n\n::: colunn\n<iframe src=\"img/3D_clusters.html\" width=\"100%\" height=\"400\" id=\"igraph\" scrolling=\"no\" seamless=\"seamless\" frameBorder=\"0\">\n\n</iframe>\n\n-   [On higher dimensions, machines are more likely to win]{.fatline}\n:::\n:::\n\n## Calinski-Harabasz index (or variance ratio) {.small}\n\n-   The Calinski-Harabasz index of a clustering is the ratio of the\n    between-cluster variance (which is essentially the variance of all\n    the cluster centroids from the dataset’s grand centroid) to the\n    total within-cluster variance (basically, the average WSS of the\n    clusters in the clustering).\n-   For a given dataset, the total sum of squares (TSS) is the squared\n    distance of all the data points from the dataset’s centroid.\n-   The TSS is independent of the clustering.\n-   If WSS(k) is the total WSS of a clustering with k clusters, then the\n    between sum of squares BSS(k) of the clustering is given by BSS(k) =\n    TSS - WSS(k).\n-   WSS(k) measures how close the points in a cluster are to each other.\n\n## Calinski-Harabasz index (or variance ratio) {.small}\n\n-   BSS(k) measures how far apart the clusters are from each other.\n-   A good clustering has a small WSS(k) and a large BSS(k).\n-   The within-cluster variance W is given by WSS(k)/(n-k), where n is\n    the number of points in the dataset.\n-   The between-cluster variance B is given by BSS(k)/(k-1).\n-   The within-cluster variance will decrease as k increases; the rate\n    of decrease should slow down past the optimal k.\n-   The between-cluster variance will increase as k, but the rate of\n    increase should slow down past the optimal k.\n-   So in theory, the ratio of B to W should be maximized at the optimal\n    k.\n\n## Gap statistic\n\n::: callout-important\nThe gap statistic is an attempt to automate the “elbow finding” on the\nWSS curve. It works best when thedata comes from a mix of populations\nthat all have approximately Gaussian distributions (a mixture of\nGaussian). - @Tibshirani2001\n:::\n\n## Silhouette {.class}\n\nSilhouette scores are defined for each sample as:\n$$s_n=(b_n-a_n)/max\\{a_n,b_n\\}$$\n\n-   where\n-   $a_n$ mean distance between object n and other objects in its\n    cluster\n-   $b_n$ mean distance between object n and objects in the nearest\n    cluster\n\n::: blockquote\n####Advantages: \\* The scores are bounded \\[-1,1\\] \\* Because we have\none score per sample, we can reallocate specific objects to better\nclusters \\* Clusters with average $s_n \\approx 0$ are overlapping, and\ncould be merged \\* We can use $s_n$ to derive a distribution of scores,\nand make inference (p-values). For example, we can compute the t-value,\n$s=E[s_n]/\\sqrt{V[s_n]}$\n:::\n\n# Examples of cluster scoring\n\n## Elbow method\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Elbow method (look at the knee)\n# Elbow method for kmeans\nf3c3<-monte(seed = 123,nvar = 3,nclus = 3,clus.size = c(100,100,100),eta2 = c(0.8,0.8,0.8))[['data']] %>% as.tibble()\nfviz_nbclust(f3c3, kmeans, method = \"wss\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Gap statistic\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Elbow method (look at the knee)\n# Elbow method for kmeans\nfviz_nbclust(f3c3, kmeans, method = \"gap_stat\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Silhouette\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_nbclust(f3c3, kmeans, method = \"silhouette\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Inference\n\n-   Interestingly, both the gap-statistic and the elbow method (which\n    uses the denominator of the CHI) suggest a optimal cluster number of\n    3, while the silhouette method suggests 2.\n-   2 is not an unreasonable choice as there was some overlap in the\n    three clusters three features sample\n\n## Use case: Factor Investing/Relative Value Strategies {.small}\n\n-   Factor investing attempts to price assets that share some common\n    characteristics\n-   Traditionally, economists group assets according to a single\n    characteristic\n-   E.g.: value, size, momentum, quality, liquidity, carry, etc.\n-   This misses known interaction effects, such as value vs. momentum,\n    and hierarchical dependencies\n-   A natural solution is to cluster assets on multiple characteristics\n    (features), and let the algorithm find the optimal number of\n    clusters\n-   We can then evaluate the performance of each cluster, and assess\n    whether the risk-premium is statistically significant\n-   This approach is also useful for relative value strategies\n\n## Exam type questions {.small}\n\n### Clustering UK asset pricing factors\n\n::: columns\n::: column\n-   [Source](https://reshare.ukdataservice.ac.uk/852704/1/dailyfactors.zip)\n-   Load the UK asset pricing factors from the UK data service\n-   The dataset contains 5 factors: market, size, value, momentum, and\n    liquidity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactors<-fml::daily_factors %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 7,389\nColumns: 7\n$ date <date> 1988-10-03, 1988-10-04, 1988-10-05, 1988-10-06, 1988-10-07, 1988…\n$ rm   <dbl> -0.0108511, 0.0021296, 0.0103932, 0.0063450, 0.0036179, 0.0001750…\n$ rf   <dbl> 0.0004220, 0.0004233, 0.0004251, 0.0004254, 0.0004254, 0.0004223,…\n$ rmrf <dbl> -0.01127306, 0.00170627, 0.00996818, 0.00591960, 0.00319251, -0.0…\n$ smb  <dbl> 0.00525148, -0.00085576, -0.00446347, -0.00249881, 0.00030991, 0.…\n$ hml  <dbl> 0.00068780, -0.00139754, 0.00140606, 0.00250139, -0.00277563, -0.…\n$ umd  <dbl> -0.00508378, 0.00121570, -0.00373774, -0.00266541, 0.00224610, 0.…\n```\n\n\n:::\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\nfactors_scaled<-scale(factors[,-1]) %>% as.tibble()\nfactors_scaled %>%\n  ggplot(aes(x=umd,y=hml)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-9-1.png){width=384}\n:::\n:::\n\n:::\n:::\n\n## Exam type questions {.small}\n\n### ONC using silhoutte method\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactors_scaled %>% select(hml,umd)->f2\nfviz_nbclust(factors_scaled %>% select(hml,umd), kmeans, method = \"silhouette\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## Exam type questions {.small}\n\n### kmean clustering results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans(f2,centers=5)->kmobj\nfactoextra::fviz_cluster(kmobj,data = f2) + labs(y='Momentum=Up minus Down', x=\"Value=High minus Low\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n## Exam type questions {.small}\n\n### Inference\n\n-   Unlike the traditional econometric interpretation the clustering has\n    found a fifth cluster.\n-   *The two asset pricing risk factors each have two components so\n    finding at least four clusters is not surprising*\n-   This fifth cluster is a combination of the four components that make\n    up the other factors.\n-   To set an relative value strategy, the quantitative analyst could\n    design an algorithm to identity this cluster and assess whether\n    their is significant risk premium in investment.\n\n## Input to clustering: Observations matrix $X$ {.small}\n\n-   In FML it is not advising to simple pass the raw observation matrix\n    to a cluster algorithm.\n-   Some treated version of the raw data is used as the input (For\n    example the information-theoretic treats in the last lecture)\n-   Most finance problems involve a correlation matrix which we will\n    focus on.\n-   Assume that we .bold\\[observe\\] N variables that follow a\n    multivariate Normal distribution characterized by a correlation\n    matrix $\\rho$ where $\\rho_{i,j}$ is the correlation between\n    variables i and j.\n-   If a strong common component is present, it is advisable to remove\n    it by applying the detoning, because a factor exposure shared by all\n    variables may hide the existence of partly shared exposures.\n\n## Correlation clustering {.small}\n\n- Correlation clustering can follow three approaches:\n\n1.  Circumvent the $X$ matrix, by directly defining the distance metrics\n    as $d_{\\rho}$ or $d_{|\\rho|}$ from last lecture.\n2.  Use the correlation matrix as $X$\n3.  Derive the $X$ matrix or a similar transformation\n    $X_{i,j}=\\sqrt{1/2\\left(1- \\rho_{i,j}\\right)}$ (the distance of\n    distances approach).\n\n::: blockquote\n-   The advantage of options 2 and 3 is that the distance between two\n    variables will be a function of multiple correlation estimates, and\n    not only one, which makes the analysis more robust to the presence\n    of outliers.\n-   A further advantage of 3 is that it acknowledges that a change from\n    correlation from 0.9 to 1.0 is greater than a change from 0.1 to\n    0:2.\n-   @Lopez2020 recommends approach 3\n:::\n\n## Distance of distances clustering algorithm {.small}\n\n-   The clustering of correlation matrices is peculiar in the sense that\n    the features match the observations: we try to group observations\n    where the observations themselves are the features (hence the\n    symmetry of X).\n\n-   Matrix X appears to be a distance matrix, but it is not. It is still\n    an observations matrix, on which distances can be evaluated.\n\n-   For large matrices X, generally it is good practice to reduce its\n    dimension via PCA.\n\n-   The idea is to replace X with its standardized orthogonal projection\n    onto a lower-dimensional space, where the number of dimensions is\n    given by the number of eigenvalues in X’s correlation matrix that\n    exceed $\\lambda_{+}$\n\n## Useful resources\n\n-   [`cluster`](https://www.rdocumentation.org/packages/cluster)\n\n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}